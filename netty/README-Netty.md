自定义时，判断是否需要firechannelread及release

netty中动态添加handler

自定义兼容多种Protobuf协议的编解码器

自动重连, 心跳检查

segmentfault  永顺  netty

https://blog.csdn.net/yangguosb/article/details/79175986

https://blog.csdn.net/yangguosb/article/details/79185799

```Netty中如何解决select空轮询导致cpu使用率升至100%的bug```

# 1.socket
```
client 到 server 端的连接是一个四元组
【源IP，源端口，目的IP，目的端口】
创建的连接就是 socket，连接创建之后需要开辟(内存)资源，如：send-q, recv-q

```

# 2.tcp 三次握手与四次挥手


## 2.99 验证三次握手与四次挥手：
```
方式1：
Linux端，
>> step1: 在 xsheel 窗口1 执行命令，监听 eth0网卡(根据你的机器填写即可)，端口是 80
>tcpdump -nn -i eth0 port 80
>> step2: 在 xsheel 窗口2 执行命令，请求 带有 80端口的服务，如：
>curl www.baidu.com:80
>> step3: 去 xsheel 窗口1 观察抓到的数据包进行分析即可
>
```


# 8.进程和线程
## 8.1 进程
```
1.基本概念：
比如windows里运行的QQ，微信等程序，他们直接不会共享内存数据。
是一个具有一定独立功能的【程序】关于某个数据集合的
【一次运行活动】，是系统进行【资源分配核调度】的基本单位。
其实就是操作系统给进程分配了一定的内存。

2.进程的结构：
>> 控制块(PCB, Process Control Block)：
进程唯一标识，OS 根据 PCB 来对进程进行控制和管理。
PCB 在进程新建时创建，并常驻内存，是进程实体的一部分。
>> 数据段：
数据段，存放原始数据，中间数据
>> 程序段：
存放在文本区域，可以被多个进程共享
```

## 8.2 线程
```
1.基本概念
线程就是进程中运行的多个子任务，是操作系统调用的最小单元。
比如QQ可以边视频，边传文件，而传文件和视频就是线程，
线程之间是可以共享内存数据的，进程可以看做是线程的容器
操作系统的线程提升了操作系统的并发性。
Java线程和操作系统的线程是1:1的关系。
其实就是操作系统给在进程空间中给线程分配了一定的内存。

2.线程的结构：
>> 控制块(TCB, Thread Control Block)：
线程唯一标识
>> 数据段：
数据段，存放原始数据，中间数据
>> 程序段：
存放在文本区域，可以被多个线程共享。
```

## 8.3 线程分类
```
我们根据计算机基础知识可知，
内存分【用户空间】和【系统空间】，系统空间是给【操作系统】使用的，用户空间是【应用程序】使用的，
应用程序如果需要访问系统空间，需要进行【系统调用】，从用户态切换到内核态。

那么线程是否也分为：用户态线程和内核态线程呢？
是的。
```

### 8.3.1 用户态的线程
```
核心：【在操作系统看来，每一个进程只有一个线程。】

第一阶段:
其实早期的时候，操作系统是没有线程的概念，线程是后面加进来的，操作系统刚开始只有进程，
操作系统分配资源的最小单位是进程，进程与进程之间相关隔离，
每个进程有自己的内存空间，文件描述符，CPU调度以进程作为最小调度单元。

第二阶段:
初期的多线程，线程是在用户空间下实现的。
什么意思?
我们都知道内存分用户空间和系统空间，系统空间是给操作系统使用的，
用户空间是应用程序使用的，应用程序如果需要访问系统空间，
需要进行系统调用，从用户态切换到内核态。

那怎么在用户空间实现的多线程呢?
实际上是【操作系统按进程维度】来调度，操作系统是不去管你用户线程的切换的，
【应用程序】自己在【用户空间实现线程的创建、维护和调度】。
【用户级线程模型】如下图:
-------------------------------------------------------------
进程A								进程B		

T1	T2	T3						T4	T5	T6
										
用户空间线程调度					用户空间线程调度		

【用户空间】
-------------------------------------------------------------
【内核空间】
CPU --> 线程
-------------------------------------------------------------
当线程在用户空间下实现时，操作系统对线程的存在一无所知，
操作系统只能看到进程，而不能看到线程。
所有的线程都是在用户空间实现。
在操作系统看来，每一个进程只有一个线程。

这种模式的优点和缺点都非常明显:
>> 缺点: 
因为操作系统不知道线程的存在，CPU的时间片切换是以进程为维度的，
如果进程中有某个线程进行了某些耗时长的操作，会阻塞整个进程。
另外当一个进程中的某一个线程(绿色线程)进行系统调用时，
比如网络IO、缺页中断等操作而导致线程阻塞，操作系统也会阻塞整个进程，
即使这个进程中其它线程还在工作。
>> 优点: 
使用库函数来实现的线程切换，就免去了用户态到内核态的切换，
这个味道熟不熟，对了，Go的协程就有借鉴了一部分这个思想。
```

### 8.3.2 内核态的线程
```
【注意】
此模型中，线程控制块 TCB 在内核空间，程序段和数据段在用户空间
此模型下，应用程序进行线程的切换时(如 T1 切换到 T2)，执行的流程是：
用户态 T1 切换(需要进行【系统调用】)到 内核态 K1，
内核态 K1 切换(CPU进行线程调度)到 内核态 K2，
内核态 K2 切换(设置程序状态字PSW)到 用户态 T2。
切换是基于OS的【中断】机制来实现的。
其中从用户态切换到内核态或者从内核态切换到用户态是比较耗费时间的，涉及到【系统调用】。
这就意味着，当创建的线程非常多的时候，会导致时间都浪费在上下文切换上，程序出现问题。

在 Java1.2 之后. Linux中的JVM是基于pthread实现的, 
可以直接说 Java 线程就是依赖操作系统实现的，是1:1的关系。

现在的Java中线程的本质，其实就是操作系统中的线程。

【内核级线程模型】如下图:
-------------------------------------------------------------
【用户空间】
进程A								进程B		

T1	T2	T3						T4	T5	T6
-------------------------------------------------------------
K1	K2	K3						K4	K5	K6 
(每一个用户线程都对应一个内核线程，此处 K1-K6即是java线程，Java:new Thread 创建的是内核线程)
                线程调度
    CPU                             CPU
【内核空间】
-------------------------------------------------------------

我们知道，每个线程都有它自己的线程上下文，
线程上下文包括线程的ID、栈、程序计数器、通用的寄存器等的合集。

线程有自己的独立的上下文，由操作系统调度，但是也有一个缺点，
那就是线程消耗资源太大了，例如在linux上，一个线程默认的栈大小是1M，
单机创建几万个线程就有点吃力了。
所以后来在编程语言的层面上，就出现了协程这个东西。

协程的模式有点类似结合了上面二种方式，即是在用户态做线程资源切换，
也让操作系统在内核层做线程调度。

协程跟操作系统的线程是有映射关系的，
例如我们建了m个协程，需要在N个线程上执行，这就是m: n的方案，
这n个线程也是靠操作系统调度实现。

另外协程是按需使用栈内存的，所以理论上可以轻轻松松创建百万级的协程。
目前协程这块支持的最好的是go语言, 不过现在OpenJDK社区也正在为JDK增加协程的支持。
```

## 8.99 CPU 时间片调度
```
>> 线程是CPU调度的基本单位。
>> 进程是CPU分配资源的基本单位。
>> CPU时间片是直接分配给线程的，线程拿到CPU时间片就能执行了。
>> CPU时间片不是先分给进程然后再由进程分给进程下的线程的。
>> 所有的进程并行，线程并行都是看起来是并行，其实都是CPU片轮换使用。
>> 线程分到了CPU时间片，就可以认为这个线程所属的进程在运行，这样就看起来是进程并行。线程也一样。
>> 某个线程的 CPU 时间片执行完之后，如果还没执行完毕，会被扔到队尾等待下次调用。
```


# 9.内核态和用户态
## 9.1 基本概念
```
现代操作系统都采用进程的概念，为了更好的处理系统的并发性、共享性等，
并使进程能够协调地工作，仅依靠计算机硬件提供的功能是远远不够的。
例如，进程的调度就不能用硬件来实现，必须使用一组基本软件对硬件资源进行改造，
以便为进程的执行提供良好的运行环境，这个软件就是内核（kernel）。

简单来说，「内核就是操作系统中的一组程序模块」，
作为可信软件来提供支持进程并发执行的基本功能和基本操作，
「具有访问硬件设备和所有内存空间的权限」。
不夸张的说，内核是操作系统的核心。

那么既然内核是程序，它需要运行，就必须被分配 CPU。
因此，CPU 上会运行两种程序，
一种是操作系统的内核程序（也称为系统程序），
一种是应用程序。
前者完成系统任务，后者实现应用任务。
两者之间有控制和被控制的关系，前者有权管理和分配资源，而后者只能向系统申请使用资源。

一、操作系统需要两种CPU状态
>> 内核态（Kernel Mode）：
运行操作系统程序，操作硬件
>> 用户态（User Mode）：
运行用户程序

二、指令划分
>> 特权指令：
只能由操作系统使用、用户程序不能使用的指令。 
举例：启动I/O 内存清零 修改程序状态字 设置时钟 允许/禁止终端 停机

>> 非特权指令：
用户程序可以使用的指令。 
举例：控制转移 算数运算 取数指令 访管指令（使用户程序从用户态陷入内核态）

三、特权级别
>> 特权环：R0、R1、R2和R3
R0相当于内核态，R3相当于用户态；
不同级别能够运行不同的指令集合；

四、CPU状态之间的转换
>> 用户态--->内核态：
唯一途径是通过中断、异常、陷入机制（访管指令）
>> 内核态--->用户态：
设置程序状态字PSW

五、内核态与用户态的区别
>> 内核态与用户态是操作系统的两种运行级别，
当程序运行在3级特权级上时，就可以称之为运行在用户态。
因为这是最低特权级，是普通的用户进程运行的特权级，
大部分用户直接面对的程序都是运行在用户态；
>> 当程序运行在0级特权级上时，就可以称之为运行在内核态。
>> 运行在用户态下的程序不能直接访问操作系统内核数据结构和程序。
当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，
在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态（比如操作硬件）。
>> 这两种状态的主要差别是
处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理器是可被抢占的。
处于内核态执行时，则能访问所有的内存空间和对象，且所占有的处理器是不允许被抢占的。

六、通常来说，以下三种情况会导致用户态到内核态的切换
这3种方式是系统在运行时由用户态转到内核态的最主要方式，
其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。
1、系统调用
这是用户态进程主动要求切换到内核态的一种方式，
用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作。
比如前例中fork()实际上就是执行了一个创建新进程的系统调用。
而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。
用户程序通常调用库函数，由库函数再调用系统调用，
因此有的库函数会使用户程序进入内核态（只要库函数中某处调用了系统调用），有的则不会。
2、异常
当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，
这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。
3、外围设备的中断
当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，
这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，
如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。
比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。
```

## 9.99 零拷贝
https://mp.weixin.qq.com/s?__biz=MzAxNzU3NjcxOA==&mid=2650735121&idx=1&sn=97c022bd00125971d19b18d700fa372d&chksm=83e8c470b49f4d668d118177e5f780e7b6ffc77a73b55193c4833035abc71e14bc7a6ff953ad&scene=27

https://blog.csdn.net/weixin_39406430/article/details/123715072

```
注意事项：
除了 Direct I/O，与磁盘相关的文件读写操作都有使用到 page cache 技术。

1.拷贝：
是指数据从一个存储区域转移到另一个存储区域。
如：
数据从硬盘拷贝到内核空间是一次拷贝。
数据从内核空间拷贝到用户空间也是一次拷贝。

2.零拷贝：
指的是拷贝数据的次数为0。
即不需要将数据从一个存储区域复制到另一个存储区域。
```

### 9.99.1 传统 IO 进行一次读写需要拷贝的次数
```
一次读写涉及到 4 次拷贝：
读数据：网卡/硬盘 --【DMA拷贝】--> 内核空间(read buffer)  --【CPU拷贝】--> 用户空间
写数据：用户空间 --【CPU拷贝】--> 内核空间(socket buffer)  --【DMA拷贝】--> 网卡/硬盘

#传统意义的拷贝
是在发送数据的时候，传统的实现方式是：
File.read(bytes)
Socket.send(bytes)
Socket.write(bytes)
File.write(bytes)

这种方式需要四次数据拷贝和四次上下文切换：
>> 数据从磁盘读取到内核的 read buffer
>> 数据从内核缓冲区拷贝到用户缓冲区
>> 数据从用户缓冲区拷贝到内核的 socket buffer
>> 数据从内核的socket buffer拷贝到网卡接口（硬件）的缓冲区

#零拷贝的概念
明显上面的第二步和第三步是没有必要的，通过 java 的 FileChannel.transferTo 方法，
可以避免上面两次多余的拷贝（当然这需要底层操作系统支持）
>> 调用 transferTo,数据从文件由 DMA 引擎拷贝到内核 read buffer
>> 接着 DMA 从内核 read buffer 将数据拷贝到网卡接口buffer
上面的两次操作都不需要CPU参与，所以就达到了零拷贝。
```

### 9.99.2 Netty中的零拷贝
```
主要体现在三个方面：

1、bytebuffer
Netty 发送和接收消息主要使用 ByteBuffer，
ByteBuffer 使用堆外内存（DirectMemory）直接进行 Socket 读写。
一次读写涉及到的读写过程(3次拷贝)：
读数据：网卡/硬盘 --【DMA拷贝】--> 内核空间(read buffer)  -- mmap 映射到 用户空间
写数据：用户空间 通过 mmap --【CPU拷贝】--> 内核空间(socket buffer)  --【DMA拷贝】--> 网卡/硬盘

原因：如果使用传统的堆内存进行Socket读写，JVM会将
堆内存buffer拷贝一份到直接内存中然后再写入socket，多了一次缓冲区的内存拷贝。
DirectMemory中可以直接通过DMA发送到网卡接口。

2、Composite Buffers
传统的 ByteBuffer，如果需要将两个 ByteBuffer 中的数据组合到一起，
我们需要首先创建一个 size = size1 + size2 大小的新的数组，然后将两个数组中的数据拷贝到新的数组中。
但是使用 Netty 提供的组合 ByteBuf，就可以避免这样的操作，
因为 CompositeByteBuf 并没有真正将多个 Buffer 组合起来，而是保存了它们的引用，
从而避免了数据的拷贝，实现了零拷贝。

3、对于 FileChannel.transferTo 的使用
Netty 中使用了 FileChannel 的 transferTo 方法，该方法依赖于操作系统实现零拷贝。
```


# 10.关于 IO ----------------------------------------------------------
## 10.0 前置知识
```
1.计算机组成原理(硬件：cpu，内存，网卡，硬盘，键鼠；软件：OS等)
2.计算机开机过程：
>> 将“内核”(即内核程序)从硬盘加载到内存，
之后会有一块安全的“内核空间”，此时内存中除了“内核空间”之外的空间称之为“用户空间”
>> 
```


## 10.1 分类
网络IO 和 文件IO
## 10.2 应用程序的 IO 交互原理
其实是应该程序与本机的 RECV-Q(接收队列) 和 SEND-Q(发送队列) 进行交互，
可以通过 netstat -nltp 查看
## 10.3 非阻塞IO
```
如 com/zy/spring/mildware/netty/nio/demo03/NonBlockNioDemo01.java:85 所示，
非阻塞的表现是：
当有新的连接进来时，代码不会卡在 原生的 serverSocketChannel.accept() 这里，
而是继续向下运行，只是新建连接时返回 -1 或者 null。
当然，这里 java 的 NIO 用 isAcceptable 方法来判断是否可以 accept。
注意：在非阻塞的场景下，一个线程可以完成大量的工作，极大的提升了效率。

```

## 10.40 多路复用器
```
操作系统层面的多路复用器
select, poll
epoll

jdk 层面的多路复用器
selector
```

## 10.98 同步与异步
```
如果是应用程序自己读取 IO，那么无论是 BIO, NIO, 还是多路复用器(poll,select)，统称为同步IO模型。

如果是操作系统通知应用程序来读取 IO，那么这样的 BIO, NIO 或多路复用器(epoll)，则是异步IO模型。
```

## 10.99 优势与劣势
```
优势：
规避多线程的问题，C10K问题

劣势：
假设真的有 1w 个连接，但是只有一个连接发送了数据，
那么每循环一次，就要向内核发送 1w 次系统调用，其中 9999 次是无效的，浪费了时间和资源。
(此处是用户空间向内核空间的循环遍历，复杂度在系统调用上)
因此多路复用器诞生了。
```


# 99.常见问题 ---------------------------------------------------------
## 99.1 两台机器之间最多可以建立多少个 TCP 连接
结论：理论上是：65535 * 65535 个
原因：由于 client 到 server 的连接是一个四元组 【源IP，源端口，目的IP，目的端口】
根据排列组合原理，理论上，最多建立 65535 * 65535 个连接

## 99.2 Netty 解决粘包拆包问题
前置：Netty 解决的是内存中的粘包拆包问题，不是 TCP 连接中粘包拆包问题

## 99.3 操作系统最多可以创建的最大线程数
### 99.3.1 windows
```
默认情况下，一个线程的栈要预留1M的内存空间 
而一个进程中可用的内存空间只有2G，所以理论上一个进程中最多可以开2048个线程 
但是内存当然不可能完全拿来作线程的栈，所以实际数目要比这个值要小。 
你也可以通过连接时修改默认栈大小，将其改的比较小，这样就可以多开一些线程。 
如将默认栈的大小改成512K，这样理论上最多就可以开4096个线程。 

即使物理内存再大，一个进程中可以起的线程总要受到2GB这个内存空间的限制。 
比方说你的机器装了64GB物理内存，但每个进程的内存空间还是4GB，其中用户态可用的还是2GB。 
```

### 99.3.2 linux
```
linux系统为每个程序分配4GB的虚拟内存，其中用户空间的虚拟内存为3GB。

正常情况下，我们使用 ulimit -s 查看系统的[线程默认栈空间大小]，默认是 8192 即 8M，
那么当我们创建线程的时候，Linux 系统会为每个线程分配独立的调用栈，也就是8MB。

由上可知，最大可使用虚拟内存是3GB左右，创建一个线程的虚拟内存消耗是8MB，
那么最大可创建的线程数为 3GB / 8MB = 384， 也就是最大能创建384个线程。

但是实际上并不能创建这么多的线程:
1 是3GB的虚拟内存，分为不同的部分，比如0x00000000 ~ 0x08048000的地址为保留区；
当程序运行时候，代码段、数据段、BSS段也会被加载进虚拟内存，也会占用一部分虚拟内存。
实际上可供使用的堆栈空间远远小于3GB。
2 是我们在线程中的一些操作，比如malloc等，会消耗堆栈空间，
导致一个线程的虚拟内存消耗大于创建时候的8MB。

在上面说，ulimit -s默认为8MB大约最大可创建384个线程，
也可以修改对栈的限制，来增大或减小创建的线程数目。
```

# 参考资料
https://www.51cto.com/article/641015.html (java线程与OS线程)
https://blog.csdn.net/guorui_java/article/details/113827888 (用户态和内核态)
https://zhuanlan.zhihu.com/p/474022823 (java线程与OS线程)
https://www.cnblogs.com/royfans/p/10147608.html (WINDOWS操作系统中可以允许最大的线程数)
https://blog.csdn.net/u011003120/article/details/122170538 (Linux下最大能创建多少线程？)