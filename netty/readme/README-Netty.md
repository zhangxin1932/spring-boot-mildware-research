自定义时，判断是否需要firechannelread及release

netty中动态添加handler

自定义兼容多种Protobuf协议的编解码器

自动重连, 心跳检查

segmentfault  永顺  netty

```Netty中如何解决select空轮询导致cpu使用率升至100%的bug```

# 1.socket
```
client 到 server 端的连接是一个四元组
【源IP，源端口，目的IP，目的端口】
创建的连接就是 socket，连接创建之后需要开辟(内存)资源，如：send-q, recv-q

```

# 2.tcp 三次握手与四次挥手


## 2.99 验证三次握手与四次挥手：
```
方式1：
Linux端，
>> step1: 在 xsheel 窗口1 执行命令，监听 eth0网卡(根据你的机器填写即可)，端口是 80
>tcpdump -nn -i eth0 port 80
>> step2: 在 xsheel 窗口2 执行命令，请求 带有 80端口的服务，如：
>curl www.baidu.com:80
>> step3: 去 xsheel 窗口1 观察抓到的数据包进行分析即可
>
```



# 7.Netty 中相关参数含义及常见配置

## 7.1 Netty之TCP参数设置

```
#TCP_NODELAY
>> 解释：
是否启用Nagle算法，改算法将小的碎片数据连接成更大的报文来最小化所发送的报文的数量。
>> 使用建议：
如果需要发送一些较小的报文，则需要禁用该算法，从而最小化报文传输延时。
只有在网络通信非常大时（通常指已经到100k+/秒了），设置为false会有些许优势，
因此建议大部分情况下均应设置为true。

#SO_LINGER
>> 解释： 
Socket参数，关闭Socket的延迟时间，默认值为-1，表示禁用该功能。
-1表示socket.close()方法立即返回，但OS底层会将发送缓冲区全部发送到对端。
0表示socket.close()方法立即返回，OS放弃发送缓冲区的数据直接向对端发送RST包，对端收到复位错误。
非0整数值表示调用socket.close()方法的线程被阻塞直到延迟时间到或发送缓冲区中的数据发送完毕，
若超时，则对端会收到复位错误。
>> 使用建议： 
使用默认值，不做设置。

#SO_SNDBUF
>> 解释： 
Socket参数，TCP数据发送缓冲区大小，即TCP发送滑动窗口，linux操作系统可使用命令：
cat /proc/sys/net/ipv4/tcp_smem查询其大小。
缓冲区的大小决定了网络通信的吞吐量（网络吞吐量=缓冲区大小/网络时延）。
>> 使用建议： 
缓冲区大小设为网络吞吐量达到带宽上限时的值，即缓冲区大小=网络带宽*网络时延。
以千兆网卡为例进行计算，假设网络时延为1ms，缓冲区大小=1000Mb/s * 1ms = 128KB。

#SO_RCVBUF
与SO_SNDBUF同理。

#SO_REUSEADDR
>> 解释：
是否复用处于TIME_WAIT状态连接的端口，
适用于有大量处于TIME_WAIT状态连接的场景，如高并发量的Http短连接场景等。

#SO_BACKLOG
>> 解释： 
Backlog主要是指当ServerSocket还没执行accept时，
这个时候的请求会放在os层面的一个队列里，这个队列的大小即为backlog值，这个参数对于大量连接涌入的场景非常重要，
例如服务端重启，所有客户端自动重连，瞬间就会涌入很多连接，
如backlog不够大的话，可能会造成客户端接到连接失败的状况，再次重连，
结果就会导致服务端一直处理不过来（当然，客户端重连最好是采用类似tcp的自动退让策略）；
>> 使用建议： 
backlog的默认值为os对应的net.core.somaxconn，
调整backlog队列的大小一定要确认 ulimit -n 中允许打开的文件数是够的。

#SO_KEEPALIVE
>> 解释：
是否使用TCP的心跳机制；
>> 使用建议： 
心跳机制由应用层自己实现。
```





# 8.进程和线程

## 8.1 进程
```
1.基本概念：
比如windows里运行的QQ，微信等程序，他们直接不会共享内存数据。
是一个具有一定独立功能的【程序】关于某个数据集合的
【一次运行活动】，是系统进行【资源分配核调度】的基本单位。
其实就是操作系统给进程分配了一定的内存。

2.进程的结构：
>> 控制块(PCB, Process Control Block)：
进程唯一标识，OS 根据 PCB 来对进程进行控制和管理。
PCB 在进程新建时创建，并常驻内存，是进程实体的一部分。
>> 数据段：
数据段，存放原始数据，中间数据
>> 程序段：
存放在文本区域，可以被多个进程共享
```

## 8.2 线程
```
1.基本概念
线程就是进程中运行的多个子任务，是操作系统调用的最小单元。
比如QQ可以边视频，边传文件，而传文件和视频就是线程，
线程之间是可以共享内存数据的，进程可以看做是线程的容器
操作系统的线程提升了操作系统的并发性。
Java线程和操作系统的线程是1:1的关系。
其实就是操作系统给在进程空间中给线程分配了一定的内存。

2.线程的结构：
>> 控制块(TCB, Thread Control Block)：
线程唯一标识
>> 数据段：
数据段，存放原始数据，中间数据
>> 程序段：
存放在文本区域，可以被多个线程共享。
```

## 8.3 线程分类
```
我们根据计算机基础知识可知，
内存分【用户空间】和【系统空间】，系统空间是给【操作系统】使用的，用户空间是【应用程序】使用的，
应用程序如果需要访问系统空间，需要进行【系统调用】，从用户态切换到内核态。

那么线程是否也分为：用户态线程和内核态线程呢？
是的。
```

### 8.3.1 用户态的线程
```
核心：【在操作系统看来，每一个进程只有一个线程。】

第一阶段:
其实早期的时候，操作系统是没有线程的概念，线程是后面加进来的，操作系统刚开始只有进程，
操作系统分配资源的最小单位是进程，进程与进程之间相关隔离，
每个进程有自己的内存空间，文件描述符，CPU调度以进程作为最小调度单元。

第二阶段:
初期的多线程，线程是在用户空间下实现的。
什么意思?
我们都知道内存分用户空间和系统空间，系统空间是给操作系统使用的，
用户空间是应用程序使用的，应用程序如果需要访问系统空间，
需要进行系统调用，从用户态切换到内核态。

那怎么在用户空间实现的多线程呢?
实际上是【操作系统按进程维度】来调度，操作系统是不去管你用户线程的切换的，
【应用程序】自己在【用户空间实现线程的创建、维护和调度】。
【用户级线程模型】如下图:
-------------------------------------------------------------
进程A								进程B		

T1	T2	T3						T4	T5	T6
										
用户空间线程调度					用户空间线程调度		

【用户空间】
-------------------------------------------------------------
【内核空间】
CPU --> 线程
-------------------------------------------------------------
当线程在用户空间下实现时，操作系统对线程的存在一无所知，
操作系统只能看到进程，而不能看到线程。
所有的线程都是在用户空间实现。
在操作系统看来，每一个进程只有一个线程。

这种模式的优点和缺点都非常明显:
>> 缺点: 
因为操作系统不知道线程的存在，CPU的时间片切换是以进程为维度的，
如果进程中有某个线程进行了某些耗时长的操作，会阻塞整个进程。
另外当一个进程中的某一个线程(绿色线程)进行系统调用时，
比如网络IO、缺页中断等操作而导致线程阻塞，操作系统也会阻塞整个进程，
即使这个进程中其它线程还在工作。
>> 优点: 
使用库函数来实现的线程切换，就免去了用户态到内核态的切换，
这个味道熟不熟，对了，Go的协程就有借鉴了一部分这个思想。
```

### 8.3.2 内核态的线程
```
【注意】
此模型中，线程控制块 TCB 在内核空间，程序段和数据段在用户空间
此模型下，应用程序进行线程的切换时(如 T1 切换到 T2)，执行的流程是：
用户态 T1 切换(需要进行【系统调用】)到 内核态 K1，
内核态 K1 切换(CPU进行线程调度)到 内核态 K2，
内核态 K2 切换(设置程序状态字PSW)到 用户态 T2。
切换是基于OS的【中断】机制来实现的。
其中从用户态切换到内核态或者从内核态切换到用户态是比较耗费时间的，涉及到【系统调用】。
这就意味着，当创建的线程非常多的时候，会导致时间都浪费在上下文切换上，程序出现问题。

在 Java1.2 之后. Linux中的JVM是基于pthread实现的, 
可以直接说 Java 线程就是依赖操作系统实现的，是1:1的关系。

现在的Java中线程的本质，其实就是操作系统中的线程。

【内核级线程模型】如下图:
-------------------------------------------------------------
【用户空间】
进程A								进程B		

T1	T2	T3						T4	T5	T6
-------------------------------------------------------------
K1	K2	K3						K4	K5	K6 
(每一个用户线程都对应一个内核线程，此处 K1-K6即是java线程，Java:new Thread 创建的是内核线程)
                线程调度
    CPU                             CPU
【内核空间】
-------------------------------------------------------------

我们知道，每个线程都有它自己的线程上下文，
线程上下文包括线程的ID、栈、程序计数器、通用的寄存器等的合集。

线程有自己的独立的上下文，由操作系统调度，但是也有一个缺点，
那就是线程消耗资源太大了，例如在linux上，一个线程默认的栈大小是1M，
单机创建几万个线程就有点吃力了。
所以后来在编程语言的层面上，就出现了协程这个东西。

协程的模式有点类似结合了上面二种方式，即是在用户态做线程资源切换，
也让操作系统在内核层做线程调度。

协程跟操作系统的线程是有映射关系的，
例如我们建了m个协程，需要在N个线程上执行，这就是m: n的方案，
这n个线程也是靠操作系统调度实现。

另外协程是按需使用栈内存的，所以理论上可以轻轻松松创建百万级的协程。
目前协程这块支持的最好的是go语言, 不过现在OpenJDK社区也正在为JDK增加协程的支持。
```

## 8.99 CPU 时间片调度
```
>> 线程是CPU调度的基本单位。
>> 进程是CPU分配资源的基本单位。
>> CPU时间片是直接分配给线程的，线程拿到CPU时间片就能执行了。
>> CPU时间片不是先分给进程然后再由进程分给进程下的线程的。
>> 所有的进程并行，线程并行都是看起来是并行，其实都是CPU片轮换使用。
>> 线程分到了CPU时间片，就可以认为这个线程所属的进程在运行，这样就看起来是进程并行。线程也一样。
>> 某个线程的 CPU 时间片执行完之后，如果还没执行完毕，会被扔到队尾等待下次调用。
```


# 9.内核态和用户态
## 9.1 基本概念
```
现代操作系统都采用进程的概念，为了更好的处理系统的并发性、共享性等，
并使进程能够协调地工作，仅依靠计算机硬件提供的功能是远远不够的。
例如，进程的调度就不能用硬件来实现，必须使用一组基本软件对硬件资源进行改造，
以便为进程的执行提供良好的运行环境，这个软件就是内核（kernel）。

简单来说，「内核就是操作系统中的一组程序模块」，
作为可信软件来提供支持进程并发执行的基本功能和基本操作，
「具有访问硬件设备和所有内存空间的权限」。
不夸张的说，内核是操作系统的核心。

那么既然内核是程序，它需要运行，就必须被分配 CPU。
因此，CPU 上会运行两种程序，
一种是操作系统的内核程序（也称为系统程序），
一种是应用程序。
前者完成系统任务，后者实现应用任务。
两者之间有控制和被控制的关系，前者有权管理和分配资源，而后者只能向系统申请使用资源。

一、操作系统需要两种CPU状态
>> 内核态（Kernel Mode）：
运行操作系统程序，操作硬件
>> 用户态（User Mode）：
运行用户程序
与之对应，操作系统的内存也分为两种状态：用户空间和内核空间。

二、指令划分
>> 特权指令：
只能由操作系统使用、用户程序不能使用的指令。 
举例：启动I/O 内存清零 修改程序状态字 设置时钟 允许/禁止终端 停机

>> 非特权指令：
用户程序可以使用的指令。 
举例：控制转移 算数运算 取数指令 访管指令（使用户程序从用户态陷入内核态）

三、特权级别
>> 特权环：R0、R1、R2和R3
R0相当于内核态，R3相当于用户态；
不同级别能够运行不同的指令集合；

四、CPU状态之间的转换
>> 用户态--->内核态：
唯一途径是通过中断、异常、陷入机制（访管指令）
>> 内核态--->用户态：
设置程序状态字PSW

五、内核态与用户态的区别
>> 内核态与用户态是操作系统的两种运行级别，
当程序运行在3级特权级上时，就可以称之为运行在用户态。
因为这是最低特权级，是普通的用户进程运行的特权级，
大部分用户直接面对的程序都是运行在用户态；
>> 当程序运行在0级特权级上时，就可以称之为运行在内核态。
>> 运行在用户态下的程序不能直接访问操作系统内核数据结构和程序。
当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，
在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态（比如操作硬件）。
>> 这两种状态的主要差别是
处于用户态执行时，进程所能访问的内存空间和对象受到限制，其所处于占有的处理器是可被抢占的。
处于内核态执行时，则能访问所有的内存空间和对象，且所占有的处理器是不允许被抢占的。

六、通常来说，以下三种情况会导致用户态到内核态的切换
这3种方式是系统在运行时由用户态转到内核态的最主要方式，
其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。
1、系统调用
这是用户态进程主动要求切换到内核态的一种方式，
用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作。
比如前例中fork()实际上就是执行了一个创建新进程的系统调用。
而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。
用户程序通常调用库函数，由库函数再调用系统调用，
因此有的库函数会使用户程序进入内核态（只要库函数中某处调用了系统调用），有的则不会。
2、异常
当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，
这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。
3、外围设备的中断
当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，
这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，
如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。
比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。
```

# 10.零拷贝
## 10.1 基本概念
```
注意事项：
除了 Direct I/O，与磁盘相关的文件读写操作都有使用到 page cache 技术。
注意看下 page cache 技术与 read buffer 是否有区别。

1.拷贝：
是指数据从一个存储区域转移到另一个存储区域。
如：
数据从硬盘拷贝到内核空间是一次拷贝。
数据从内核空间拷贝到用户空间也是一次拷贝。

2.零拷贝：
零拷贝技术是一个思想，指的是指计算机执行操作时，
CPU 不需要先将数据从某处内存复制到另一个特定区域。

可见，零拷贝的特点是 CPU 不全程负责内存中的数据写入其他组件，CPU 仅仅起到管理的作用。
但注意，零拷贝不是不进行拷贝，而是 CPU 不再全程负责数据拷贝时的搬运工作。
如果数据本身不在内存中，那么必须先通过某种方式拷贝到内存中（这个过程 CPU 可以不参与），
因为数据只有在内存中，才能被转移，才能被 CPU 直接读取计算。

3.Linux 的零拷贝技术有多种实现策略，但根据策略可以分为如下几种类型：
>> 减少甚至避免用户空间和内核空间之间的数据拷贝：
在一些场景下，用户进程在数据传输过程中并不需要对数据进行访问和处理，
那么数据在 Linux 的 Page Cache 和用户进程的缓冲区之间的传输就完全可以避免，
让数据拷贝完全在内核里进行，甚至可以通过更巧妙的方式避免在内核里的数据拷贝。
这一类实现一般是是通过增加新的系统调用来完成的，
比如 Linux 中的 mmap()，sendfile() 以及 splice() 等。
>> 绕过内核的直接 I/O：
允许在用户态进程绕过内核直接和硬件进行数据传输，内核在传输过程中只负责一些管理和辅助的工作。
这种方式其实和第一种有点类似，也是试图避免用户空间和内核空间之间的数据传输，
只是第一种方式是把数据传输过程放在内核态完成，
而这种方式则是直接绕过内核和硬件通信，效果类似但原理完全不同。
>> 内核缓冲区和用户缓冲区之间的传输优化：
这种方式侧重于在用户进程的缓冲区和操作系统的页缓存之间的 CPU 拷贝的优化。
这种方法延续了以往那种传统的通信方式，但更灵活。
```

## 10.2 零拷贝涉及的技术
```
零拷贝技术的具体实现方式有很多，例如：
>> sendfile
>> mmap
>> splice
>> 直接 Direct I/O
不同的零拷贝技术适用于不同的应用场景。

前瞻性总结：
>> DMA 技术回顾：
DMA 负责内存与其他组件之间的数据拷贝，CPU 仅需负责管理，而无需负责全程的数据拷贝；
>> 使用 page cache 的 zero copy：
sendfile：
一次代替 read/write 系统调用，通过使用 DMA 技术以及传递文件描述符，实现了 zero copy
mmap：
仅代替 read 系统调用，将内核空间地址映射为用户空间地址，write 操作直接作用于内核空间。
通过 DMA 技术以及地址映射技术，用户空间与内核空间无须数据拷贝，实现了 zero copy。
>> 不使用 page cache 的 Direct I/O：
读写操作直接在磁盘上进行，不使用 page cache 机制，通常结合用户空间的用户缓存使用。
通过 DMA 技术直接与磁盘/网卡进行数据交互，实现了 zero copy。
```

### 10.2.1 sendfile
```
sendfile 的应用场景是：
用户从磁盘读取一些文件数据后不需要经过任何计算与处理就通过网络传输出去。
此场景的典型应用是【消息队列】。
sendfile 主要使用到了两个技术：
>> DMA 技术；
>> 传递文件描述符代替数据拷贝。
我们需要注意 sendfile 系统调用的局限性。
如果应用程序需要对从磁盘读取的数据进行写操作，例如解密或加密，
那么 sendfile 系统调用就完全没法用。
这是因为用户线程根本就不能够通过 sendfile 系统调用得到传输的数据。

1.DMA 技术
DMA 技术很容易理解，本质上，DMA 技术就是我们在主板上放一块独立的芯片。
在进行内存和 I/O 设备的数据传输的时候，我们不再通过 CPU 来控制数据传输，
而直接通过 DMA 控制器（DMA Controller，简称 DMAC）。
这块芯片，我们可以认为它其实就是一个协处理器（Co-Processor）。

DMA 负责磁盘到内核空间中的 Page cache（read buffer）的数据拷贝
以及从内核空间中的 socket buffer 到网卡的数据拷贝。

DMAC 最有价值的地方体现在，当我们要传输的数据特别大、速度特别快，
或者传输的数据特别小、速度特别慢的时候。

比如说，我们用千兆网卡或者硬盘传输大量数据的时候，
如果都用 CPU 来搬运的话，肯定忙不过来，所以可以选择 DMAC。
而当数据传输很慢的时候，DMAC 可以等数据到齐了，
再发送信号，给到 CPU 去处理，而不是让 CPU 在那里忙等待。

注意，这里面的“协”字。
DMAC 是在“协助”CPU，完成对应的数据传输工作。
在 DMAC 控制数据传输的过程中，我们还是需要 CPU 的进行控制，
但是具体数据的拷贝不再由 CPU 来完成。

但是 DMA 有其局限性，DMA 仅仅能用于设备之间交换数据时进行数据拷贝，
但是设备内部的数据拷贝还需要 CPU 进行，
例如 CPU 需要负责内核空间数据与用户空间数据之间的拷贝（内存内部的拷贝）。

2.传递文件描述符代替数据拷贝
传递文件描述可以代替数据拷贝，这是由于两个原因：
>> page cache 以及 socket buffer 都在内核空间中；
>> 数据传输过程前后没有任何写操作。
注意事项：
只有网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术
才可以通过传递文件描述符的方式避免内核空间内的一次 CPU 拷贝。
这意味着此优化取决于 Linux 系统的物理网卡是否支持
（Linux 在内核 2.4 版本里引入了 DMA 的 scatter/gather -- 分散/收集功能，
只要确保 Linux 版本高于 2.4 即可）。
```

![](D:\0000_code\zx\spring-boot-mildware-research\netty\readme\sendfile利用DMA技术的读写过程.jpg)





### 10.2.2 mmap

```
参考下述文章
```

https://spongecaptain.cool/SimpleClearFileIO/3.%20mmap.html

### 10.2.3 Direct I/O
#### 10.2.3.1 概述
```
Direct I/O 即直接 I/O。
用户空间直接通过 DMA 的方式与磁盘以及网卡进行数据拷贝。
其名字中的“直接”二字用于区分使用 page cache 机制的缓存 I/O。
>> 缓存文件 I/O：
用户空间要读写一个文件并不直接与磁盘交互，而是中间夹了一层缓存，即 page cache；
>> 直接文件 I/O：
用户空间读取的文件直接与磁盘交互，没有中间 page cache 层。

“直接”在这里还有另一层语义：
其他所有技术中，数据至少需要在内核空间存储一份，
但是在 Direct I/O 技术中，数据直接存储在用户空间中，绕过了内核。

#Direct I/O 的读写非常有特点：
>> Write 操作：
由于其不使用 page cache，
所以其进行写文件，如果返回成功，
数据就真的落盘了（不考虑磁盘自带的缓存）；
>> Read 操作：
由于其不使用 page cache，
每次读操作是真的从磁盘中读取，不会从文件系统的缓存中读取。

事实上，即使 Direct I/O 还是可能需要使用操作系统的 fsync 系统调用。为什么？
这是因为虽然文件的数据本身没有使用任何缓存，
但是文件的元数据仍然需要缓存，包括 VFS 中的 inode cache 和 dentry cache 等。

在部分操作系统中，在 Direct I/O 模式下进行 write 系统调用能够确保文件数据落盘，
但是文件元数据不一定落盘。如果在此类操作系统上，
那么还需要执行一次 fsync 系统调用确保文件元数据也落盘。
否则，可能会导致文件异常、元数据确实等情况。
MySQL 的 O_DIRECT 与 O_DIRECT_NO_FSYNC 配置是一个具体案例。
```



![](D:\0000_code\zx\spring-boot-mildware-research\netty\readme\Direct IO 模式一次读写涉及的过程.jpg)



#### 10.2.3.2 Direct I/O 的优缺点

```
优点：
>> Linux 中的直接 I/O 技术省略掉缓存 I/O 技术中操作系统内核缓冲区的使用，
数据直接在应用程序地址空间和磁盘之间进行传输，
从而使得自缓存应用程序可以省略掉复杂的系统级别的缓存结构，
而执行程序自己定义的数据读写管理，从而降低系统级别的管理对应用程序访问数据的影响。
>> 与其他零拷贝技术一样，避免了内核空间到用户空间的数据拷贝，
如果要传输的数据量很大，使用直接 I/O 的方式进行数据传输，
而不需要操作系统内核地址空间拷贝数据操作的参与，这将会大大提高性能。

缺点：
>> 由于设备之间的数据传输是通过 DMA 完成的，
因此用户空间的数据缓冲区内存页必须进行 page pinning（页锁定），
这是为了防止其物理页框地址被交换到磁盘或者被移动到新的地址
而导致 DMA 去拷贝数据的时候在指定的地址找不到内存页从而引发缺页错误，
而页锁定的开销并不比 CPU 拷贝小，所以为了避免频繁的页锁定系统调用，
应用程序必须分配和注册一个持久的内存池，用于数据缓冲。
>> 如果访问的数据不在应用程序缓存中，
那么每次数据都会直接从磁盘进行加载，这种直接加载会非常缓慢。
>> 在应用层引入直接 I/O 需要应用层自己管理，这带来了额外的系统复杂性。
```

#### 10.2.3.3 谁会使用 Direct I/O？
```
【自缓存应用程序】

对于某些应用程序来说，它会有它自己的数据缓存机制，
比如，它会将数据缓存在应用程序地址空间，
这类应用程序完全不需要使用操作系统内核中的高速缓冲存储器，
这类应用程序就被称作是自缓存应用程序（ self-caching applications ）。

例如，应用内部维护一个缓存空间，当有读操作时，首先读取应用层的缓存数据，
如果没有，那么就通过 Direct I/O 直接通过磁盘 I/O 来读取数据。
缓存仍然在应用，只不过应用觉得自己实现一个缓存比操作系统的缓存更高效。

数据库管理系统是这类应用程序的一个代表。
自缓存应用程序倾向于使用数据的逻辑表达方式，而非物理表达方式；
当系统内存较低的时候，自缓存应用程序会让这种数据的逻辑缓存被换出，
而并非是磁盘上实际的数据被换出。

自缓存应用程序对要操作的数据的语义了如指掌，所以它可以采用更加高效的缓存替换算法。
自缓存应用程序有可能会在多台主机之间共享一块内存，
那么自缓存应用程序就需要提供一种能够有效地将用户地址空间的缓存数据置为无效的机制，
从而确保应用程序地址空间缓存数据的一致性。

另一方面，目前 Linux 上的异步 IO 库，其依赖于文件使用 O_DIRECT 模式打开，它们通常一起配合使用。
```

#### 10.2.3.4 如何使用 Direct I/O？
```
用户应用需要实现用户空间内的缓存区，读/写操作应当尽量通过此缓存区提供。
如果有性能上的考虑，那么尽量避免频繁地基于 Direct I/O 进行读/写操作。
```

#### 10.2.3.5 典型案例
```
1.【Kakfa】
Kafka 作为一个消息队列，涉及到磁盘 I/O 主要有两个操作：
>> Provider 向 Kakfa 发送消息，Kakfa 负责将消息以日志的方式持久化落盘；
>> Consumer 向 Kakfa 进行拉取消息，Kafka 负责从磁盘中读取一批日志消息，然后再通过网卡发送。

# mmap 机制
Kakfa 服务端接收 Provider 的消息并持久化的场景下使用 mmap 机制，
能够基于顺序磁盘 I/O 提供高效的持久化能力，使用的 Java 类为 java.nio.MappedByteBuffer。
# sendfile 机制
Kakfa 服务端向 Consumer 发送消息的场景下使用 sendfile 机制，这种机制主要两个好处：
>> sendfile 避免了内核空间到用户空间的 CPU 全程负责的数据移动；
>> sendfile 基于 Page Cache 实现，因此如果有多个 Consumer 在同时消费一个主题的消息，
那么由于消息一直在 page cache 中进行了缓存，
因此只需一次磁盘 I/O，就可以服务于多个 Consumer。

【注意】
使用 mmap 来对接收到的数据进行持久化，
使用 sendfile 从持久化介质中读取数据然后对外发送是一对常用的组合。
但是注意，你无法利用 sendfile 来持久化数据，利用 mmap 来实现 CPU 全程不参与数据搬运的数据拷贝。

2.【MySQL】
```



## 10.3 传统 IO 进行一次读写需要拷贝的次数

```
一次读写涉及到 4 次拷贝：
读数据：网卡/硬盘 --【DMA拷贝】--> 内核空间(read buffer)  --【CPU拷贝】--> 用户空间
写数据：用户空间 --【CPU拷贝】--> 内核空间(socket buffer)  --【DMA拷贝】--> 网卡/硬盘

#传统意义的拷贝
是在发送数据的时候，传统的实现方式是：
File.read(bytes)
Socket.send(bytes)
Socket.write(bytes)
File.write(bytes)

这种方式需要四次数据拷贝和四次上下文切换：
4 次数据拷贝：
>> 数据从磁盘读取到内核的 read buffer
>> 数据从内核缓冲区拷贝到用户缓冲区
>> 数据从用户缓冲区拷贝到内核的 socket buffer
>> 数据从内核的socket buffer拷贝到网卡接口（硬件）的缓冲区
4 次上下文切换：
>> read 系统调用时：用户态切换到内核态；
>> read 系统调用完毕：内核态切换回用户态；
>> write 系统调用时：用户态切换到内核态；
>> write 系统调用完毕：内核态切换回用户态。

#零拷贝的概念
明显上面的第二步和第三步是没有必要的，通过 java 的 FileChannel.transferTo 方法，
可以避免上面两次多余的拷贝（当然这需要底层操作系统支持）
>> 调用 transferTo,数据从文件由 DMA 引擎拷贝到内核 read buffer
>> 接着 DMA 从内核 read buffer 将数据拷贝到网卡接口buffer
上面的两次操作都不需要CPU参与，所以就达到了零拷贝。
```

![](D:\0000_code\zx\spring-boot-mildware-research\netty\readme\传统IO一次读写涉及的过程.jpg)



## 10.4 Netty中的零拷贝

```
主要体现在三个方面：

1、bytebuffer(基于Direct IO 机制)
Netty 发送和接收消息主要使用 ByteBuffer，
ByteBuffer 使用堆外内存（DirectMemory）直接进行 Socket 读写。
一次读写涉及到的读写过程(3次拷贝)：
读数据：网卡/硬盘 --【DMA拷贝】--> 内核空间(read buffer)  -- mmap 映射到 用户空间
写数据：用户空间 通过 mmap --【CPU拷贝】--> 内核空间(socket buffer)  --【DMA拷贝】--> 网卡/硬盘

原因：如果使用传统的堆内存进行Socket读写，JVM会将
堆内存buffer从用户空间 拷贝一份到内核空间的直接内存中后再写入socket，多了一次缓冲区的内存拷贝。
DirectMemory中可以直接通过DMA发送到网卡接口。

2、Composite Buffers
传统的 ByteBuffer，如果需要将两个 ByteBuffer 中的数据组合到一起，
我们需要首先创建一个 size = size1 + size2 大小的新的数组，然后将两个数组中的数据拷贝到新的数组中。
但是使用 Netty 提供的组合 ByteBuf，就可以避免这样的操作，
因为 CompositeByteBuf 并没有真正将多个 Buffer 组合起来，而是保存了它们的引用，
从而避免了数据的拷贝，实现了零拷贝。

3、对于 FileChannel.transferTo 的使用(基于linux的sendfile机制)
Netty 中使用了 FileChannel 的 transferTo 方法，该方法依赖于操作系统实现零拷贝。
```


# 10.关于 IO
## 10.0 前置知识
```
1.计算机组成原理(硬件：cpu，内存，网卡，硬盘，键鼠；软件：OS等)
2.计算机开机过程：
>> 将“内核”(即内核程序)从硬盘加载到内存，
之后会有一块安全的“内核空间”，此时内存中除了“内核空间”之外的空间称之为“用户空间”
>> 
```


## 10.1 分类
网络IO 和 文件IO
## 10.2 应用程序的 IO 交互原理
其实是应该程序与本机的 RECV-Q(接收队列) 和 SEND-Q(发送队列) 进行交互，
可以通过 netstat -nltp 查看
## 10.3 非阻塞IO
```
如 com/zy/spring/mildware/netty/nio/demo03/NonBlockNioDemo01.java:85 所示，
非阻塞的表现是：
当有新的连接进来时，代码不会卡在 原生的 serverSocketChannel.accept() 这里，
而是继续向下运行，只是新建连接时返回 -1 或者 null。
当然，这里 java 的 NIO 用 isAcceptable 方法来判断是否可以 accept。
注意：在非阻塞的场景下，一个线程可以完成大量的工作，极大的提升了效率。

```

## 10.40 多路复用器
```
操作系统层面的多路复用器
select, poll
epoll

jdk 层面的多路复用器
selector
```

## 10.98 同步与异步
```
如果是应用程序自己读取 IO，那么无论是 BIO, NIO, 还是多路复用器(poll,select)，统称为同步IO模型。

如果是操作系统通知应用程序来读取 IO，那么这样的 BIO, NIO 或多路复用器(epoll)，则是异步IO模型。
```

## 10.99 优势与劣势
```
优势：
规避多线程的问题，C10K问题

劣势：
假设真的有 1w 个连接，但是只有一个连接发送了数据，
那么每循环一次，就要向内核发送 1w 次系统调用，其中 9999 次是无效的，浪费了时间和资源。
(此处是用户空间向内核空间的循环遍历，复杂度在系统调用上)
因此多路复用器诞生了。
```


# 99.常见问题
## 99.1 两台机器之间最多可以建立多少个 TCP 连接
结论：理论上是：65535 * 65535 个
原因：由于 client 到 server 的连接是一个四元组 【源IP，源端口，目的IP，目的端口】
根据排列组合原理，理论上，最多建立 65535 * 65535 个连接

## 99.2 Netty 解决粘包拆包问题
前置：Netty 解决的是内存中的粘包拆包问题，不是 TCP 连接中粘包拆包问题

## 99.3 操作系统最多可以创建的最大线程数
### 99.3.1 windows
```
默认情况下，一个线程的栈要预留1M的内存空间 
而一个进程中可用的内存空间只有2G，所以理论上一个进程中最多可以开2048个线程 
但是内存当然不可能完全拿来作线程的栈，所以实际数目要比这个值要小。 
你也可以通过连接时修改默认栈大小，将其改的比较小，这样就可以多开一些线程。 
如将默认栈的大小改成512K，这样理论上最多就可以开4096个线程。 

即使物理内存再大，一个进程中可以起的线程总要受到2GB这个内存空间的限制。 
比方说你的机器装了64GB物理内存，但每个进程的内存空间还是4GB，其中用户态可用的还是2GB。 
```

### 99.3.2 linux
```
linux系统为每个程序分配4GB的虚拟内存，其中用户空间的虚拟内存为3GB。

正常情况下，我们使用 ulimit -s 查看系统的[线程默认栈空间大小]，默认是 8192 即 8M，
那么当我们创建线程的时候，Linux 系统会为每个线程分配独立的调用栈，也就是8MB。

由上可知，最大可使用虚拟内存是3GB左右，创建一个线程的虚拟内存消耗是8MB，
那么最大可创建的线程数为 3GB / 8MB = 384， 也就是最大能创建384个线程。

但是实际上并不能创建这么多的线程:
1 是3GB的虚拟内存，分为不同的部分，比如0x00000000 ~ 0x08048000的地址为保留区；
当程序运行时候，代码段、数据段、BSS段也会被加载进虚拟内存，也会占用一部分虚拟内存。
实际上可供使用的堆栈空间远远小于3GB。
2 是我们在线程中的一些操作，比如malloc等，会消耗堆栈空间，
导致一个线程的虚拟内存消耗大于创建时候的8MB。

在上面说，ulimit -s默认为8MB大约最大可创建384个线程，
也可以修改对栈的限制，来增大或减小创建的线程数目。
```

# 参考资料
https://www.51cto.com/article/641015.html (java线程与OS线程)
https://blog.csdn.net/guorui_java/article/details/113827888 (用户态和内核态)
https://zhuanlan.zhihu.com/p/474022823 (java线程与OS线程)
https://www.cnblogs.com/royfans/p/10147608.html (WINDOWS操作系统中可以允许最大的线程数)
https://blog.csdn.net/u011003120/article/details/122170538 (Linux下最大能创建多少线程？)

https://mp.weixin.qq.com/s?__biz=MzAxNzU3NjcxOA==&mid=2650735121&idx=1&sn=97c022bd00125971d19b18d700fa372d&chksm=83e8c470b49f4d668d118177e5f780e7b6ffc77a73b55193c4833035abc71e14bc7a6ff953ad&scene=27(零拷贝)

https://blog.csdn.net/weixin_39406430/article/details/123715072(零拷贝)

https://blog.csdn.net/yangguosb/article/details/79185799(Netty之TCP参数设置)

https://netty.io/wiki/reference-counted-objects.html (官网)



